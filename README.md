# Vision Transformers vs CNNs on Low-Resolution Datasets

This project compares the performance of various Vision Transformer and CNN architectures on low-resolution image classification tasks. The study includes multiple datasets, resolutions, and evaluation metrics to provide a comprehensive analysis.

## Features

- **Multiple Architectures**:
  - CNN Models:
    - LeNet-5
    - Modified ResNet-18
    - EfficientNetV2-S
    - MobileNetV2
    - ConvMixer
    - RepVGG-A0
  - Transformer Models:
    - Vision Transformer (ViT)
    - DeiT-Tiny
    - MobileViT
    - Tiny Swin Transformer
    - CvT (Convolutional vision Transformer)

- **Datasets**:
  - CIFAR-10 (32x32, 16x16, 8x8)
  - Fashion-MNIST (28x28, 14x14, 7x7)
  - SVHN (32x32, 16x16, 8x8)

- **Evaluation Metrics**:
  - Classification accuracy
  - Model size
  - FLOPs
  - Inference time
  - Noise robustness
  - Confusion matrices

- **Visualizations**:
  - Training curves
  - Attention maps
  - Feature maps
  - Confusion matrices
  - Model comparison charts
  - Noise robustness plots

- **LaTeX Tables**:
  - Model comparison tables
  - Resolution comparison tables
  - Noise robustness tables

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/vision-transformers-vs-cnns-lowres.git
cd vision-transformers-vs-cnns-lowres
```

2. Create a virtual environment and activate it:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

1. Configure the experiment:
   - Edit `configs/experiment_config.yaml` to specify:
     - Datasets and resolutions
     - Models to evaluate
     - Training parameters
     - Evaluation metrics
     - Visualization settings

2. Run the experiments:
```bash
python train.py
```

3. Generate visualizations:
```bash
python utils/visualize.py
```

4. Generate LaTeX tables:
```bash
python utils/generate_tables.py
```

## Project Structure

```
.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiment_config.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ datasets.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ cnn/
â”‚   â”‚   â”œâ”€â”€ lenet5.py
â”‚   â”‚   â”œâ”€â”€ resnet.py
â”‚   â”‚   â”œâ”€â”€ efficientnet.py
â”‚   â”‚   â”œâ”€â”€ mobilenetv2.py
â”‚   â”‚   â”œâ”€â”€ convmixer.py
â”‚   â”‚   â””â”€â”€ repvgg.py
â”‚   â””â”€â”€ transformer/
â”‚       â”œâ”€â”€ vit.py
â”‚       â”œâ”€â”€ deit.py
â”‚       â”œâ”€â”€ mobilevit.py
â”‚       â”œâ”€â”€ swin.py
â”‚       â””â”€â”€ cvt.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ generate_tables.py
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Results

The results of the experiments will be saved in the following directories:
- `results/`: JSON files containing experiment results
- `visualizations/`: Generated plots and visualizations
- `tables/`: LaTeX tables for the results

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- The implementation of models is based on their respective papers and official repositories
- Special thanks to the PyTorch community for their excellent documentation and examples

## ğŸ“‹ Project Overview

This research investigates the performance of Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) on low-resolution image datasets, with a focus on:
- Performance comparison across different input resolutions (8Ã—8, 16Ã—16, 32Ã—32)
- Model efficiency metrics (size, speed, FLOPs)
- Training dynamics and convergence
- Robustness to noise and data corruption

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ data/                    # Dataset loading and preprocessing
â”œâ”€â”€ models/                  # Model architectures
â”‚   â”œâ”€â”€ cnn/                # CNN implementations
â”‚   â””â”€â”€ transformer/        # Transformer implementations
â”œâ”€â”€ utils/                  # Utility functions
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ evaluate.py            # Evaluation script
â”œâ”€â”€ visualize.py           # Visualization utilities
â””â”€â”€ requirements.txt       # Project dependencies
```

## ğŸš€ Getting Started

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Download datasets:
```bash
python data/download_datasets.py
```

3. Train models:
```bash
python train.py --config configs/experiment_config.yaml
```

4. Evaluate models:
```bash
python evaluate.py --config configs/experiment_config.yaml
```

## ğŸ“Š Datasets

- CIFAR-10 (32Ã—32, downsampled to 16Ã—16, 8Ã—8)
- Fashion-MNIST (28Ã—28, downsampled to 14Ã—14, 8Ã—8)
- SVHN (32Ã—32, downsampled to 16Ã—16, 8Ã—8)

## ğŸ§  Models

### CNN Models
- LeNet-5
- ResNet-18 (modified)
- EfficientNetV2-S
- MobileNetV2
- ConvMixer
- RepVGG-A0

### Transformer Models
- ViT-Tiny
- DeiT-Tiny
- MobileViT (Tiny & XXS)
- Tiny Swin Transformer
- CvT

## ğŸ“ˆ Evaluation Metrics

- Classification Accuracy
- Model Size
- FLOPs
- Training Time
- Inference Time
- Convergence Analysis
- Robustness to Noise
- Visualization (CAM/Attention Maps)

## ğŸ“ Citation

If you use this code in your research, please cite:
```bibtex
@article{vision-transformers-vs-cnns-lowres,
  title={An Empirical Comparison of Vision Transformers and Convolutional Neural Networks on Low-Resolution Datasets for Efficient Visual Recognition},
  author={[Your Name]},
  journal={[Journal Name]},
  year={2024}
}
``` 